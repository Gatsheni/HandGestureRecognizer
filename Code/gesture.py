# -*- coding: utf-8 -*-
"""Gesture.ipynb

Automatically generated by Colaboratory.

"""

#importing all the dependencies
import torch
import pandas as pd
import numpy as np
from google.colab import files
from torch.autograd import Variable
import torch.nn.functional as F
from torch import nn, autograd, optim
import io
import torch.utils.data
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from torchvision import transforms
from torchvision import datasets

#loading the data from local machine to Colab and preparing it.
uploaded = files.upload()

train_df = pd.read_csv(io.BytesIO(uploaded['train_data.csv']))

train_labels=train_df.label.values

train_features=train_df.loc[:,train_df.columns != "label"]

train_features=train_features.values

train_features_np = np.array(train_features, dtype = 'float32')

train_labels_np = np.array(train_labels, dtype = 'float32')

#spliting the data into 20% testing and 80% training data
training_features, testing_features, training_target, testing_larget = train_test_split(train_features_np, train_labels_np, test_size=0.2, random_state = 42)

#turning the data into torch tensors
train_feature_set = torch.from_numpy(training_features)
train_label_set = torch.from_numpy(training_target).type(torch.LongTensor)

test_features_set = torch.from_numpy(testing_features)
test_target_set = torch.from_numpy(testing_larget).type(torch.LongTensor)

#wrapping the data into torch dataset and dataloaders
training_dataset = torch.utils.data.TensorDataset(train_feature_set,train_label_set)
testing_dataset = torch.utils.data.TensorDataset(test_features_set,test_target_set)

training_loader = torch.utils.data.DataLoader(training_dataset, batch_size = 100, shuffle = False)
testing_loader = torch.utils.data.DataLoader(testing_dataset, batch_size = 100, shuffle = False)

#Building the CNN class
class CNN(nn.Module):
  def __init__(self):
    super().__init__()
    self.conv1 = nn.Conv2d(1, 16, kernel_size = 5)
    self.conv2 = nn.Conv2d(16, 32, kernel_size = 5)
    self.conv2_drop = nn.Dropout2d()
    self.fc1 = nn.Linear(32*4*4, 100)
    self.fc2 = nn.Linear(100, 25)
  #forward fucntion that feeds data to the next layer  
  def forward(self, x_in):
    x_in = F.relu(F.max_pool2d(self.conv1(x_in), 2))
    x_in = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x_in)), 2))
    x_in = x_in.view(x_in.size(0), -1)
    x_in = F.relu(self.fc1(x_in))
    x_in = F.dropout(x_in, training = self.training)
    x_in = self.fc2(x_in)
    return F.log_softmax(x_in)

#Defining the fit function that will perform the training and validation of the model
def fit (epoch, model, data_loader, phase = 'training', volatile = False):
  if phase == 'training':
      model.train()
  if phase == 'validation':
      model.eval()
      volatile = True
  running_loss = 0.0
  running_correct = 0
  for i, data in enumerate(data_loader, 1):
        features,target = data
        features = Variable(features.view(-1,1,28,28))
        target = Variable(target)
        if phase == 'training':
            optimizer.zero_grad()
        output = model(features)
        target=target.long()
        target=target.squeeze()
        loss = F.nll_loss(output,target)
        running_loss += F.nll_loss(output,target, size_average=False).data
        preds= output.data.max(dim=1, keepdim=True)[1]
        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()
        if phase == 'training':
          loss.backward()
          optimizer.step()
  loss = running_loss/len(data_loader.dataset)
  accuracy = 100.* running_correct/len(data_loader.dataset)
  print (f'{phase} loss is {loss: } and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy: }')
  return loss,accuracy

#initializing the model
model = CNN()


#optimizer used in training SGD
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum = 0.5)
train_losses , train_accuracy = [], []
val_losses , val_accuracy = [], []
for epoch in range (1, 50):
  epoch_loss, epoch_accuracy = fit(epoch,model,training_loader,phase= 'training')
  val_epoch_loss , val_epoch_accuracy = fit(epoch,model,training_loader,phase= 'validation')
  train_losses.append(epoch_loss)
  train_accuracy.append(epoch_accuracy)
  val_losses.append(val_epoch_loss)
  val_accuracy.append(val_epoch_accuracy)

#saving the model state
mod = torch.save(model.state_dict(), 'gest.pth')

#uploading the test data from local computer
uploaded = files.upload()
test1_df = pd.read_csv(io.BytesIO(uploaded['test_data.csv']))

#preparing the test data
test_set_features=test1_df.values
test_numpy = np.array(test_set_features, dtype = 'float32')
test_input = torch.from_numpy(test_numpy)

#tesing the model
Output = model(test_input.view(-1,1,28,28))

#organizing the results
prediction = torch.max(Output, 1)

#creating the csv results file to be submitted on kaggle
index=[]
for i in range(1, 7173):
    index.append(i)
num_index = np.array(index)
df = pd.DataFrame({"samp_id" : num_index, "label" :prediction[1]})
df.to_csv("result.csv", index=False)

#downloding the csv results file
files.download('result.csv')

#downloading the model's saved state
files.download('gest.pth')
